{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "46c8ad8f-a297-4bea-b4c6-40816e71cd8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Модель: llama3.2\n",
      "Вопрос: Стоит ли использовать искусственный интеллект, если с его помощью можно автоматизировать многие процессы, но он не всегда бывает стабилен?\n",
      "Оценка вопроса ollama:\n",
      "\n",
      "Пропозиция: Использовать искусственный интеллект\n",
      "Допущения: ['Многие процессы могут быть автоматизированы']\n",
      "Критерии: ['Стабильность']\n",
      "Анализ: Пропозиция использования искусственного интеллекта основана на том факте, что многие процессы можно автоматизировать, что приведет к повышению эффективности и стабильности. Однако необходимо учитывать потенциальные риски и ограничения использования ИИ, такие как потеря контроля над процессами или неопределенность в отношении результатов.\n",
      "\n",
      "Однако, если рассматривать только критерий стабильности, можно заключить, что автоматизация некоторых процессов может привести к повышению стабильности, поскольку ИИ может работать без ошибок и непредсказуемости. \n",
      "\n",
      "Следовательно, на основе данных допущений и критериев, я даю следующий ответ:\n",
      "\n",
      "VERDICT: YES\n",
      "\n",
      "ФИНАЛЬНЫЙ ВЫВОД (0/1): 1\n"
     ]
    }
   ],
   "source": [
    "import json, re, time\n",
    "from pathlib import Path\n",
    "import ollama\n",
    "\n",
    "# создание в рабочей директории папки для сохранения результатов\n",
    "Path(\"outputs\").mkdir(exist_ok=True)\n",
    "# подготовка файлов для логирования результатов работы программы\n",
    "trace_path = Path(\"outputs/yn_trace.jsonl\")\n",
    "result_path = Path(\"outputs/yn_result.md\")\n",
    "# открыть файл трассировки и записать пустую строку\n",
    "trace_path.write_text(\"\", encoding=\"utf-8\")  # эффективно очищает файл, если он существовал, или создаёт новый пустой\n",
    "ts = time.strftime(\"%Y-%m-%d %H:%M:%S\")      # временная метка\n",
    "# выбор модели\n",
    "model_name = \"llama3.2\"\n",
    "temperature = 0.2            \n",
    "top_p = 0.9\n",
    "seed = 8\n",
    "num_predict = 200\n",
    "print(\"Модель:\", model_name)\n",
    "\n",
    "\n",
    "\n",
    "#____ВОПРОС____\n",
    "question = \"Стоит ли использовать искусственный интеллект, если с его помощью можно автоматизировать многие процессы, но он не всегда бывает стабилен?\"\n",
    "print(\"Вопрос:\", question)\n",
    "\n",
    "\n",
    "\n",
    "#____ПЕРВИЧНАЯ ОЦЕНКА ВОПРОСА ollama____\n",
    "prompt_A = (\n",
    "    \"Сохранив суть вопроса, преобразуй его в более чёткий вопрос - такой, на который можно ответить да/нет и корректно выдели отдельно допущения и критерии вопроса.\\n\"\n",
    "    \"Верни строго один JSON-объект без текста до/после, поля:\\n\"\n",
    "    \"{ \\\"proposition\\\": <строка>, \\\"assumptions\\\": [строки], \\\"criteria\\\": [строки] }.\\n\"\n",
    "    \"Вопрос:\\n\\n\"\n",
    "    + question\n",
    ")\n",
    "\n",
    "resp_A = ollama.generate(\n",
    "    model=model_name,\n",
    "    prompt=prompt_A,\n",
    "    options={\"temperature\": temperature, \"top_p\": top_p, \"seed\": seed, \"num_predict\": num_predict}\n",
    ")\n",
    "text_A = resp_A[\"response\"].strip()\n",
    "\n",
    "# распарсить JSON; при неудаче — более строгий повтор\n",
    "ok_A = False # флаг успешного парсинга\n",
    "attempt_a = 0  # счётчик попыток\n",
    "while not ok_A and attempt_a < 3:\n",
    "    attempt_a += 1\n",
    "    try:\n",
    "        data_A = json.loads(text_A)\n",
    "        ok_A = True\n",
    "    except Exception as e:\n",
    "        print(\"Не JSON, более строгий повтор:...\")\n",
    "        prompt_A_strict = (\n",
    "            \"СТРОГО верни валидный JSON без лишних символов. Формат:\\n\"\n",
    "            \"{ \\\"proposition\\\": <строка>, \\\"assumptions\\\": [строки], \\\"criteria\\\": [строки] }.\\n\\nВопрос:\\n\" + text_A\n",
    "        )\n",
    "        resp_A = ollama.generate(\n",
    "            model=model_name,\n",
    "            prompt=prompt_A_strict,\n",
    "            options={\"temperature\": 0.0, \"top_p\": 1.0, \"seed\": seed+attempt_a, \"num_predict\": 200}\n",
    "        ) # seed: seed+attempt - изменённый сид для разнообразия\n",
    "        text_A = resp_A[\"response\"].strip()\n",
    "\n",
    "# если не получен успешный ответ, используется типизированная заготовка\n",
    "if not ok_A:\n",
    "    data_A = {\"proposition\": question, \"assumptions\": [\"Наиболее упрощённое рассуждение.\"], \"criteria\": [\"Базовый ответ.\"]}\n",
    "\n",
    "print(\"Оценка вопроса ollama:\")\n",
    "print(\"\\nПропозиция:\", data_A.get(\"proposition\",\"\"))\n",
    "print(\"Допущения:\", data_A.get(\"assumptions\",[]))\n",
    "print(\"Критерии:\", data_A.get(\"criteria\",[]))\n",
    "\n",
    "# логирование в файл трассировки\n",
    "with open(trace_path, \"a\", encoding=\"utf-8\") as f:\n",
    "    f.write(json.dumps({\"step\":\"A\",\"ts\":ts,\"request\":prompt_A[:2000],\"response\":text_A[:2000],\"parsed\":data_A}, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "#____АНАЛИЗ ПРОПОЗИЦИИ ПО РЕЗУЛЬТАТАМ ОЦЕНКИ ollama____\n",
    "prop, assm, crit = data_A.get(\"proposition\", question), data_A.get(\"assumptions\", []), data_A.get(\"criteria\", [])\n",
    "context_block = \"\\n\".join(filter(None, [\n",
    "    f\"Пропозиция: {prop}\",\n",
    "    f\"Допущения: {'; '.join(assm)}\" if assm else \"\",\n",
    "    f\"Критерии: {'; '.join(crit)}\" if crit else \"\"\n",
    "]))\n",
    "\n",
    "prompt_B = (\n",
    "    \"Проанализируй пропозицию да/нет кратко по-русски на 2–4 предложения, ссылаясь на критерии/допущения, \"\n",
    "    \"и завершай - VERDICT: YES или VERDICT: NO. Никаких других маркеров после вердикта.\\n\\n\"\n",
    "    + context_block\n",
    ")\n",
    "\n",
    "resp_B = ollama.generate(\n",
    "    model=model_name,\n",
    "    prompt=prompt_B,\n",
    "    options={\"temperature\": 0.2, \"top_p\": top_p, \"seed\": seed, \"num_predict\": 240}\n",
    ")\n",
    "text_B = resp_B[\"response\"].strip()\n",
    "print(text_B)\n",
    "\n",
    "with open(trace_path, \"a\", encoding=\"utf-8\") as f:\n",
    "    f.write(json.dumps({\"step\":\"B\",\"ts\":ts,\"request\":prompt_B[:2000],\"response\":text_B[:2000]}, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "#____ПРИВЕДЕНИЕ ВЕРДИКТА К БИНАРНОМУ ФОРМАТУ____\n",
    "binary = None\n",
    "verdict = None\n",
    "\n",
    "# поиск VERDICT: YES/NO\n",
    "lit = re.search(r\"VERDICT:\\s*(YES|NO)\\b\", text_B, flags=re.IGNORECASE)\n",
    "if lit:\n",
    "    verdict = lit.group(1).upper()\n",
    "\n",
    "# при неудаче повторный запрос на yes/no\n",
    "attempt_b = 0\n",
    "while verdict is None and attempt_b < 3:\n",
    "    attempt_b += 1\n",
    "    resp = ollama.generate(\n",
    "        model=model_name,\n",
    "        prompt=\"Ответь строго одним словом на английском: YES или NO. Только это слово. Пропозиция:\\n\\n\" + prop,\n",
    "        options={\"temperature\": 0.0, \"top_p\": 1.0, \"seed\": seed+attempt_b, \"num_predict\": 10}\n",
    "    )\n",
    "    txt = resp[\"response\"].strip()\n",
    "    print(\"Строгий ответ:\", txt)\n",
    "    lit = re.search(r\"\\b(YES|NO)\\b\", txt, flags=re.IGNORECASE)\n",
    "    if lit:\n",
    "        verdict = lit.group(1).upper()\n",
    "    with open(trace_path, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(json.dumps({\"step\":\"B_strict\",\"attempt\":attempt_b,\"raw\":txt}, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "# YES/NO в 1/0\n",
    "if verdict == \"YES\":\n",
    "    binary = 1\n",
    "elif verdict == \"NO\":\n",
    "    binary = 0\n",
    "\n",
    "# при неудаче повторный запрос на 1/0\n",
    "attempt_c = 0\n",
    "while binary is None and attempt_c < 3:\n",
    "    attempt_c += 1\n",
    "    rule = (\n",
    "        \"Выбери 1 (за) или 0 (против) по пропозиции выше. Если аргументы равны — выбери 0. \"\n",
    "        \"Верни строго одну цифру без текста.\"\n",
    "    )\n",
    "    resp = ollama.generate(\n",
    "        model=model_name,\n",
    "        prompt=rule + \"\\n\\nПропозиция: \" + prop,\n",
    "        options={\"temperature\": 0.0, \"top_p\": 1.0, \"seed\": seed+10+attempt_c, \"num_predict\": 5}\n",
    "    )\n",
    "    txt = resp[\"response\"].strip()\n",
    "    print(\"Строго бинар:\", txt)\n",
    "    lit_2 = re.search(r\"^(0|1)$\", txt)\n",
    "    if lit_2:\n",
    "        binary = int(lit_2.group(1))\n",
    "    with open(trace_path, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(json.dumps({\"step\":\"C_force01\",\"attempt\":attempt_c,\"raw\":txt}, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "# при неудаче - поиск по всем ответам\n",
    "if binary is None:\n",
    "    # YES/NO/ДА/НЕТ из всех ответов\n",
    "    text_all = (text_A + \"\\n\" + text_B).lower()\n",
    "    if re.search(r\"\\b(yes|да)\\b\", text_all): binary = 1\n",
    "    if re.search(r\"\\b(no|нет)\\b\", text_all): binary = 0 if binary is None else binary\n",
    "\n",
    "if binary is None:\n",
    "    # при неудаче - последняя попытка строгой оценки моделью\n",
    "    resp = ollama.generate(\n",
    "        model=model_name,\n",
    "        prompt=\"Верни строго одну цифру: 1 (за) или 0 (против) по пропозиции ниже. Без комментариев.\\n\\n\" + prop,\n",
    "        options={\"temperature\": 0.0, \"top_p\": 1.0, \"seed\": seed+99, \"num_predict\": 5}\n",
    "    )\n",
    "    txt = resp[\"response\"].strip()\n",
    "    print(\"Финальная попытка:\", txt)\n",
    "    lit_3 = re.search(r\"^(0|1)$\", txt)\n",
    "    if lit_3:\n",
    "        binary = int(lit_3.group(1))\n",
    "\n",
    "# при неудаче - дефолтное значение 0\n",
    "if binary is None:\n",
    "    binary = 0\n",
    "    print(\"Не удалось получить бинар от модели, установлено значение 0\")\n",
    "\n",
    "print(\"\\nФИНАЛЬНЫЙ ВЫВОД (0/1):\", binary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
